{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gatkins/cs895-s18/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from math import sqrt\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Based on the AutoRec paper, we look exclusively at the ratings.dat from the [Movielens](https://grouplens.org/datasets/movielens/) project 1 million ratings which has 6000 users and 4000 movies.\n",
    "\n",
    "The format for the data is: UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "For my example I exclude the Timestamp column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ratings data\n",
    "ratings = pd.read_csv('../../data/ml-1m/ratings.dat',\n",
    "                      sep=\"::\", header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     1     2     3     4     5     6     7     8     9     10    ...   3943  \\\n",
      "0                                                                 ...          \n",
      "1      5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "5      0.0   0.0   0.0   0.0   0.0   2.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6      4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "7      0.0   0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "8      4.0   0.0   0.0   3.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "9      5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "10     5.0   5.0   0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0  ...    0.0   \n",
      "11     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "12     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "13     0.0   3.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   3.0  ...    0.0   \n",
      "14     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "15     0.0   0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "16     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "17     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "18     4.0   2.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   5.0  ...    0.0   \n",
      "19     5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   5.0  ...    0.0   \n",
      "20     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "21     3.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "22     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   3.0  ...    0.0   \n",
      "23     4.0   2.0   0.0   0.0   0.0   3.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "24     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "25     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "26     3.0   0.0   2.0   3.0   5.0   0.0   4.0   0.0   0.0   0.0  ...    0.0   \n",
      "27     0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "28     3.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "29     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "30     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    5.0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
      "6011   5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   4.0  ...    0.0   \n",
      "6012   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6013   5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6014   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6015   5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6016   4.0   0.0   2.0   0.0   0.0   0.0   3.0   0.0   0.0   4.0  ...    0.0   \n",
      "6017   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6018   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6019   0.0   4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   3.0  ...    0.0   \n",
      "6020   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6021   3.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6022   5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6023   0.0   0.0   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6024   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6025   5.0   0.0   3.0   3.0   0.0   0.0   4.0   0.0   0.0   0.0  ...    0.0   \n",
      "6026   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6027   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   5.0  ...    0.0   \n",
      "6028   0.0   0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6029   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6030   0.0   4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6031   0.0   0.0   0.0   0.0   0.0   0.0   0.0   5.0   0.0   0.0  ...    0.0   \n",
      "6032   4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6033   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6034   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6035   4.0   0.0   1.0   2.0   1.0   0.0   3.0   0.0   0.0   0.0  ...    0.0   \n",
      "6036   0.0   0.0   0.0   2.0   0.0   3.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6037   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6038   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6039   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "6040   3.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "\n",
      "1     3944  3945  3946  3947  3948  3949  3950  3951  3952  \n",
      "0                                                           \n",
      "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "5      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "7      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "8      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "9      0.0   0.0   0.0   0.0   3.0   0.0   0.0   0.0   0.0  \n",
      "10     0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0   0.0  \n",
      "11     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "12     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "13     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "14     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "15     0.0   0.0   0.0   0.0   3.0   0.0   0.0   0.0   0.0  \n",
      "16     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "17     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "18     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "19     0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0   0.0  \n",
      "20     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "21     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "22     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "23     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   4.0  \n",
      "24     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "25     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "26     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "27     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "28     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "29     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "30     0.0   0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0  \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "6011   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6012   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6013   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6014   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6015   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6016   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6017   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6018   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6019   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6020   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6021   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6022   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6023   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6024   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6025   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6026   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6027   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6028   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6029   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6030   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6031   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6032   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6033   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6034   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6035   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6036   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6037   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6038   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6039   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6040   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[6040 rows x 3706 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gatkins/cs895-s18/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# create a user x movie matrix, make no rating movies = 0\n",
    "ratings_pivot = pd.pivot_table(ratings[[0, 1, 2]],\n",
    "                               values=2, index=0, columns=1).fillna(0)\n",
    "\n",
    "print(ratings_pivot)\n",
    "\n",
    "X_train, X_test = train_test_split(ratings_pivot, train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4832, 3706)\n"
     ]
    }
   ],
   "source": [
    "# Decide number of neurons for input, hidden, and output layers\n",
    "n_nodes_inpl = 3706\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_outl = 3706\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "batch_size = 100  # how many images to use together for training\n",
    "hm_epochs = 120   # how many times to go through the entire dataset\n",
    "tot_users = X_train.shape[0]  # unique users\n",
    "# we find that there are 3706 unique columns, 4832 rows\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer weights\n",
    "hidden_1_layer_vals = {'weights': tf.Variable(\n",
    "    tf.random_normal([n_nodes_inpl + 1, n_nodes_hl1]))}\n",
    "# output layer weights\n",
    "output_layer_vals = {'weights': tf.Variable(\n",
    "    tf.random_normal([n_nodes_hl1 + 1, n_nodes_outl]))}\n",
    "\n",
    "input_layer = tf.placeholder('float', [None, 3706])\n",
    "\n",
    "# add a constant node to the first layer\n",
    "# it needs to have the same shape as the input layer for me to be\n",
    "# able to concatinate it later\n",
    "input_layer_const = tf.fill([tf.shape(input_layer)[0], 1], 1.0)\n",
    "input_layer_concat = tf.concat([input_layer, input_layer_const], 1)\n",
    "# multiply output of input_layer wth a weight matrix\n",
    "layer_1 = tf.nn.sigmoid(tf.matmul(input_layer_concat,\n",
    "                                  hidden_1_layer_vals['weights']))\n",
    "# adding one bias node to the hidden layer\n",
    "layer1_const = tf.fill([tf.shape(layer_1)[0], 1], 1.0)\n",
    "layer_concat = tf.concat([layer_1, layer1_const], 1)\n",
    "# multiply output of hidden with a weight matrix to get final output\n",
    "output_layer = tf.matmul(layer_concat, output_layer_vals['weights'])\n",
    "# output_true shall have the original shape for error calculations\n",
    "output_true = tf.placeholder('float', [None, 3706])\n",
    "# define our cost function\n",
    "meansq = tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train 10.298523265310767 RMSE test 10.313349140825943\n",
      "Epoch 0 / 120 loss: 7132.146530151367\n",
      "RMSE train 9.039114118720658 RMSE test 9.066486223692962\n",
      "Epoch 1 / 120 loss: 4453.74641418457\n",
      "RMSE train 8.245614205574212 RMSE test 8.282472099757166\n",
      "Epoch 2 / 120 loss: 3590.8529510498047\n",
      "RMSE train 7.603365730259653 RMSE test 7.654883615389325\n",
      "Epoch 3 / 120 loss: 3026.9862022399902\n",
      "RMSE train 7.058516663147201 RMSE test 7.125202093327304\n",
      "Epoch 4 / 120 loss: 2590.63826751709\n",
      "RMSE train 6.587856293194937 RMSE test 6.662512561843248\n",
      "Epoch 5 / 120 loss: 2246.2126502990723\n",
      "RMSE train 6.159734669630153 RMSE test 6.245925248388828\n",
      "Epoch 6 / 120 loss: 1961.824291229248\n",
      "RMSE train 5.775151278713948 RMSE test 5.8712707800536155\n",
      "Epoch 7 / 120 loss: 1719.4101181030273\n",
      "RMSE train 5.436558996520675 RMSE test 5.545310270427774\n",
      "Epoch 8 / 120 loss: 1517.3590869903564\n",
      "RMSE train 5.143503544995957 RMSE test 5.260421958834906\n",
      "Epoch 9 / 120 loss: 1351.4754829406738\n",
      "RMSE train 4.890538155580832 RMSE test 5.014621220791182\n",
      "Epoch 10 / 120 loss: 1215.3978309631348\n",
      "RMSE train 4.677602115138345 RMSE test 4.80561508655237\n",
      "Epoch 11 / 120 loss: 1105.4651107788086\n",
      "RMSE train 4.4881785721981995 RMSE test 4.623906754474972\n",
      "Epoch 12 / 120 loss: 1014.3309020996094\n",
      "RMSE train 4.328569915029386 RMSE test 4.470800794828684\n",
      "Epoch 13 / 120 loss: 938.2585248947144\n",
      "RMSE train 4.188562695323934 RMSE test 4.336430864835487\n",
      "Epoch 14 / 120 loss: 875.7902984619141\n",
      "RMSE train 4.067773474403933 RMSE test 4.22234246500385\n",
      "Epoch 15 / 120 loss: 822.6146640777588\n",
      "RMSE train 3.9616871663292965 RMSE test 4.122126692961291\n",
      "Epoch 16 / 120 loss: 777.9506759643555\n",
      "RMSE train 3.866965523790082 RMSE test 4.032316319920105\n",
      "Epoch 17 / 120 loss: 739.2644147872925\n",
      "RMSE train 3.7824358180983317 RMSE test 3.9524241067354238\n",
      "Epoch 18 / 120 loss: 705.6969861984253\n",
      "RMSE train 3.7056094544786995 RMSE test 3.880183233315022\n",
      "Epoch 19 / 120 loss: 676.2756586074829\n",
      "RMSE train 3.635363336070803 RMSE test 3.8149113933153296\n",
      "Epoch 20 / 120 loss: 649.6834840774536\n",
      "RMSE train 3.571710182224173 RMSE test 3.7555563030315002\n",
      "Epoch 21 / 120 loss: 625.984920501709\n",
      "RMSE train 3.5133335989656254 RMSE test 3.701658890182152\n",
      "Epoch 22 / 120 loss: 604.8857955932617\n",
      "RMSE train 3.4603974663131636 RMSE test 3.6530618280164586\n",
      "Epoch 23 / 120 loss: 585.8430967330933\n",
      "RMSE train 3.4123177847622244 RMSE test 3.607690117780555\n",
      "Epoch 24 / 120 loss: 568.8920736312866\n",
      "RMSE train 3.366328066940592 RMSE test 3.565071824919675\n",
      "Epoch 25 / 120 loss: 553.4402980804443\n",
      "RMSE train 3.323528732367372 RMSE test 3.524734861267024\n",
      "Epoch 26 / 120 loss: 538.9581346511841\n",
      "RMSE train 3.2832501992346708 RMSE test 3.4872095150366933\n",
      "Epoch 27 / 120 loss: 525.5112915039062\n",
      "RMSE train 3.2464343483224303 RMSE test 3.4521819398120748\n",
      "Epoch 28 / 120 loss: 513.1248664855957\n",
      "RMSE train 3.211641294728458 RMSE test 3.418839977317135\n",
      "Epoch 29 / 120 loss: 501.8549852371216\n",
      "RMSE train 3.178746794470029 RMSE test 3.38779765629455\n",
      "Epoch 30 / 120 loss: 491.28430557250977\n",
      "RMSE train 3.1472184905161025 RMSE test 3.358015740438252\n",
      "Epoch 31 / 120 loss: 481.3582248687744\n",
      "RMSE train 3.1176449883015005 RMSE test 3.330453231370548\n",
      "Epoch 32 / 120 loss: 471.995756149292\n",
      "RMSE train 3.0899048369284348 RMSE test 3.3043900674436975\n",
      "Epoch 33 / 120 loss: 463.3015637397766\n",
      "RMSE train 3.062578848183515 RMSE test 3.279131869352664\n",
      "Epoch 34 / 120 loss: 455.15373611450195\n",
      "RMSE train 3.0372428521473656 RMSE test 3.255800435540496\n",
      "Epoch 35 / 120 loss: 447.34321212768555\n",
      "RMSE train 3.0126212443492792 RMSE test 3.2331708203286698\n",
      "Epoch 36 / 120 loss: 439.9852976799011\n",
      "RMSE train 2.9888413097058026 RMSE test 3.211615698267261\n",
      "Epoch 37 / 120 loss: 433.00976943969727\n",
      "RMSE train 2.96660240681322 RMSE test 3.1910460447840627\n",
      "Epoch 38 / 120 loss: 426.3428325653076\n",
      "RMSE train 2.9451280662238926 RMSE test 3.1714112581500316\n",
      "Epoch 39 / 120 loss: 420.01026487350464\n",
      "RMSE train 2.9246252716325265 RMSE test 3.1528925992874703\n",
      "Epoch 40 / 120 loss: 414.0165376663208\n",
      "RMSE train 2.9046507517249487 RMSE test 3.1350898668924065\n",
      "Epoch 41 / 120 loss: 408.3199577331543\n",
      "RMSE train 2.885508752011014 RMSE test 3.1180658263092305\n",
      "Epoch 42 / 120 loss: 402.865788936615\n",
      "RMSE train 2.8674913663934825 RMSE test 3.10208486986275\n",
      "Epoch 43 / 120 loss: 397.6648759841919\n",
      "RMSE train 2.8498566657714535 RMSE test 3.086503300397387\n",
      "Epoch 44 / 120 loss: 392.7677493095398\n",
      "RMSE train 2.8333431988870106 RMSE test 3.071859856501362\n",
      "Epoch 45 / 120 loss: 388.0175533294678\n",
      "RMSE train 2.817132899315146 RMSE test 3.057591130990515\n",
      "Epoch 46 / 120 loss: 383.497510433197\n",
      "RMSE train 2.8010659544821905 RMSE test 3.043551637726823\n",
      "Epoch 47 / 120 loss: 379.1529469490051\n",
      "RMSE train 2.785372787166776 RMSE test 3.029914437775254\n",
      "Epoch 48 / 120 loss: 374.8798394203186\n",
      "RMSE train 2.770543683832853 RMSE test 3.016862503605039\n",
      "Epoch 49 / 120 loss: 370.73260259628296\n",
      "RMSE train 2.7561982337565496 RMSE test 3.0040288489728635\n",
      "Epoch 50 / 120 loss: 366.79709482192993\n",
      "RMSE train 2.742880064155957 RMSE test 2.9917934049363066\n",
      "Epoch 51 / 120 loss: 363.0987205505371\n",
      "RMSE train 2.7296827865303355 RMSE test 2.979862244691186\n",
      "Epoch 52 / 120 loss: 359.5411477088928\n",
      "RMSE train 2.716329371532571 RMSE test 2.968072286015499\n",
      "Epoch 53 / 120 loss: 356.08548641204834\n",
      "RMSE train 2.7036359843604427 RMSE test 2.956821736504699\n",
      "Epoch 54 / 120 loss: 352.68758726119995\n",
      "RMSE train 2.691224605124551 RMSE test 2.9457702068892395\n",
      "Epoch 55 / 120 loss: 349.41355991363525\n",
      "RMSE train 2.6794216033557485 RMSE test 2.9351800267879757\n",
      "Epoch 56 / 120 loss: 346.21316289901733\n",
      "RMSE train 2.667386540087803 RMSE test 2.9246276117829404\n",
      "Epoch 57 / 120 loss: 343.1529474258423\n",
      "RMSE train 2.655866489161316 RMSE test 2.914608343416591\n",
      "Epoch 58 / 120 loss: 340.12646484375\n",
      "RMSE train 2.644600157461629 RMSE test 2.904743315108025\n",
      "Epoch 59 / 120 loss: 337.1726360321045\n",
      "RMSE train 2.633477301600617 RMSE test 2.895352918014166\n",
      "Epoch 60 / 120 loss: 334.31033277511597\n",
      "RMSE train 2.6227633463468067 RMSE test 2.8861650157174106\n",
      "Epoch 61 / 120 loss: 331.5175223350525\n",
      "RMSE train 2.61224055147879 RMSE test 2.8771938978169076\n",
      "Epoch 62 / 120 loss: 328.8235421180725\n",
      "RMSE train 2.6022879493257682 RMSE test 2.868473975070591\n",
      "Epoch 63 / 120 loss: 326.214382648468\n",
      "RMSE train 2.5925518081423755 RMSE test 2.860014143310735\n",
      "Epoch 64 / 120 loss: 323.7089591026306\n",
      "RMSE train 2.583229474487931 RMSE test 2.8516893652898223\n",
      "Epoch 65 / 120 loss: 321.272225856781\n",
      "RMSE train 2.573677348824744 RMSE test 2.8433286137913725\n",
      "Epoch 66 / 120 loss: 318.89196491241455\n",
      "RMSE train 2.5647923640601884 RMSE test 2.835292331065005\n",
      "Epoch 67 / 120 loss: 316.61335945129395\n",
      "RMSE train 2.555803225573963 RMSE test 2.8273293580577046\n",
      "Epoch 68 / 120 loss: 314.38093757629395\n",
      "RMSE train 2.547027827450551 RMSE test 2.8195933124082195\n",
      "Epoch 69 / 120 loss: 312.2126536369324\n",
      "RMSE train 2.5381235616215627 RMSE test 2.8119244497961695\n",
      "Epoch 70 / 120 loss: 310.03052711486816\n",
      "RMSE train 2.529658611697274 RMSE test 2.8045288423363646\n",
      "Epoch 71 / 120 loss: 307.90246295928955\n",
      "RMSE train 2.5211364536502168 RMSE test 2.79728250394942\n",
      "Epoch 72 / 120 loss: 305.85075426101685\n",
      "RMSE train 2.513067587358583 RMSE test 2.7902102504694968\n",
      "Epoch 73 / 120 loss: 303.8185224533081\n",
      "RMSE train 2.5049070354448055 RMSE test 2.783369042819498\n",
      "Epoch 74 / 120 loss: 301.8489828109741\n",
      "RMSE train 2.497134127072901 RMSE test 2.776717274269962\n",
      "Epoch 75 / 120 loss: 299.92500352859497\n",
      "RMSE train 2.4891125345189358 RMSE test 2.7701088392757587\n",
      "Epoch 76 / 120 loss: 297.98560905456543\n",
      "RMSE train 2.481775360314298 RMSE test 2.763934019493871\n",
      "Epoch 77 / 120 loss: 296.13710832595825\n",
      "RMSE train 2.4739911496933265 RMSE test 2.7576056724201865\n",
      "Epoch 78 / 120 loss: 294.3372950553894\n",
      "RMSE train 2.46694378286253 RMSE test 2.751609341993213\n",
      "Epoch 79 / 120 loss: 292.56050300598145\n",
      "RMSE train 2.4596031487082364 RMSE test 2.7455685680001154\n",
      "Epoch 80 / 120 loss: 290.84859228134155\n",
      "RMSE train 2.452734740843074 RMSE test 2.7398431287702087\n",
      "Epoch 81 / 120 loss: 289.1656160354614\n",
      "RMSE train 2.445617989303027 RMSE test 2.7340700507077997\n",
      "Epoch 82 / 120 loss: 287.52478981018066\n",
      "RMSE train 2.4390208569288463 RMSE test 2.728521883936404\n",
      "Epoch 83 / 120 loss: 285.9126868247986\n",
      "RMSE train 2.4322013188053306 RMSE test 2.7229064380891987\n",
      "Epoch 84 / 120 loss: 284.3402557373047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train 2.425276016363174 RMSE test 2.7173557237590105\n",
      "Epoch 85 / 120 loss: 282.7868094444275\n",
      "RMSE train 2.418883908899621 RMSE test 2.7119062410816275\n",
      "Epoch 86 / 120 loss: 281.23749256134033\n",
      "RMSE train 2.4123828475341824 RMSE test 2.706463629870963\n",
      "Epoch 87 / 120 loss: 279.73076915740967\n",
      "RMSE train 2.405820836275776 RMSE test 2.7012154655609524\n",
      "Epoch 88 / 120 loss: 278.23960304260254\n",
      "RMSE train 2.3997782722293985 RMSE test 2.6960013110753893\n",
      "Epoch 89 / 120 loss: 276.76927375793457\n",
      "RMSE train 2.393870594196531 RMSE test 2.690942100745125\n",
      "Epoch 90 / 120 loss: 275.37907886505127\n",
      "RMSE train 2.388132262604539 RMSE test 2.6861459582261267\n",
      "Epoch 91 / 120 loss: 274.0567479133606\n",
      "RMSE train 2.382626360269453 RMSE test 2.6813219180146826\n",
      "Epoch 92 / 120 loss: 272.75160932540894\n",
      "RMSE train 2.37694931104114 RMSE test 2.6764738453754644\n",
      "Epoch 93 / 120 loss: 271.481849193573\n",
      "RMSE train 2.3713769873977557 RMSE test 2.6717404480380047\n",
      "Epoch 94 / 120 loss: 270.2225046157837\n",
      "RMSE train 2.365910322960197 RMSE test 2.667046701764873\n",
      "Epoch 95 / 120 loss: 268.9414529800415\n",
      "RMSE train 2.3603182498772664 RMSE test 2.662340339835409\n",
      "Epoch 96 / 120 loss: 267.6923813819885\n",
      "RMSE train 2.354855264436957 RMSE test 2.657644086644339\n",
      "Epoch 97 / 120 loss: 266.45400190353394\n",
      "RMSE train 2.349574123165338 RMSE test 2.6531205533215605\n",
      "Epoch 98 / 120 loss: 265.2307085990906\n",
      "RMSE train 2.3441249881125503 RMSE test 2.6485705259463375\n",
      "Epoch 99 / 120 loss: 264.00992155075073\n",
      "RMSE train 2.3387032894870794 RMSE test 2.643960420757914\n",
      "Epoch 100 / 120 loss: 262.80200028419495\n",
      "RMSE train 2.333453245596936 RMSE test 2.6395175470838312\n",
      "Epoch 101 / 120 loss: 261.5882704257965\n",
      "RMSE train 2.328285698274821 RMSE test 2.6349900698365567\n",
      "Epoch 102 / 120 loss: 260.4053375720978\n",
      "RMSE train 2.323116821972859 RMSE test 2.6305162848294406\n",
      "Epoch 103 / 120 loss: 259.25021481513977\n",
      "RMSE train 2.318381143954812 RMSE test 2.6261749286376572\n",
      "Epoch 104 / 120 loss: 258.10776472091675\n",
      "RMSE train 2.313351125255806 RMSE test 2.6219462382555907\n",
      "Epoch 105 / 120 loss: 257.0101490020752\n",
      "RMSE train 2.308347399051884 RMSE test 2.617770196929604\n",
      "Epoch 106 / 120 loss: 255.92116045951843\n",
      "RMSE train 2.3035764621265447 RMSE test 2.6136615814430844\n",
      "Epoch 107 / 120 loss: 254.83288073539734\n",
      "RMSE train 2.298839794093777 RMSE test 2.609550875626185\n",
      "Epoch 108 / 120 loss: 253.7532377243042\n",
      "RMSE train 2.2940342285649655 RMSE test 2.6056034993706634\n",
      "Epoch 109 / 120 loss: 252.70194745063782\n",
      "RMSE train 2.2892895796947124 RMSE test 2.601717759291005\n",
      "Epoch 110 / 120 loss: 251.66419529914856\n",
      "RMSE train 2.2845283174517768 RMSE test 2.597758128838343\n",
      "Epoch 111 / 120 loss: 250.61620140075684\n",
      "RMSE train 2.2799144797053725 RMSE test 2.5938709315159887\n",
      "Epoch 112 / 120 loss: 249.5843162536621\n",
      "RMSE train 2.2753218721151844 RMSE test 2.5901018638693616\n",
      "Epoch 113 / 120 loss: 248.5858669281006\n",
      "RMSE train 2.2707175698301136 RMSE test 2.5862408297899844\n",
      "Epoch 114 / 120 loss: 247.5994517803192\n",
      "RMSE train 2.2662960537705317 RMSE test 2.5824436066152483\n",
      "Epoch 115 / 120 loss: 246.6244513988495\n",
      "RMSE train 2.261967441073991 RMSE test 2.5787399233688646\n",
      "Epoch 116 / 120 loss: 245.67302083969116\n",
      "RMSE train 2.257469440368204 RMSE test 2.575019715056892\n",
      "Epoch 117 / 120 loss: 244.72409915924072\n",
      "RMSE train 2.2531110563574637 RMSE test 2.571307782933507\n",
      "Epoch 118 / 120 loss: 243.78691172599792\n",
      "RMSE train 2.2486568328859025 RMSE test 2.567606480917731\n",
      "Epoch 119 / 120 loss: 242.8237760066986\n"
     ]
    }
   ],
   "source": [
    "# initialising variables and starting the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# train for epoch limit\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "\n",
    "    for i in range(int(tot_users / batch_size)):\n",
    "        epoch_x = X_train[i * batch_size: (i + 1) * batch_size]\n",
    "        _, c = sess.run([optimizer, meansq],\n",
    "                        feed_dict={input_layer: epoch_x,\n",
    "                                   output_true: epoch_x})\n",
    "        epoch_loss += c\n",
    "\n",
    "    output_train = sess.run(output_layer,\n",
    "                            feed_dict={input_layer: X_train})\n",
    "    output_test = sess.run(output_layer,\n",
    "                           feed_dict={input_layer: X_test})\n",
    "\n",
    "    print('RMSE train', sqrt(MSE(output_train, X_train)),\n",
    "          'RMSE test', sqrt(MSE(output_test, X_test)))\n",
    "    print('Epoch', epoch, '/', hm_epochs, 'loss:', epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fun part to find user recommendations\n",
    "# # Select titles\n",
    "# titles = pd.read_csv('../../data/ml-1m/movies.dat',\n",
    "#                      sep=\"::\", header=None, engine='python')\n",
    "# titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample User: 1\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       0.0\n",
      "4       0.0\n",
      "5       0.0\n",
      "6       4.0\n",
      "7       4.0\n",
      "8       0.0\n",
      "9       0.0\n",
      "10      0.0\n",
      "11      5.0\n",
      "12      0.0\n",
      "13      0.0\n",
      "14      4.0\n",
      "15      0.0\n",
      "16      4.0\n",
      "17      0.0\n",
      "18      0.0\n",
      "19      0.0\n",
      "20      4.0\n",
      "21      4.0\n",
      "22      0.0\n",
      "23      0.0\n",
      "24      0.0\n",
      "25      3.0\n",
      "26      0.0\n",
      "27      0.0\n",
      "28      0.0\n",
      "29      0.0\n",
      "30      0.0\n",
      "       ... \n",
      "3923    0.0\n",
      "3924    0.0\n",
      "3925    0.0\n",
      "3926    0.0\n",
      "3927    0.0\n",
      "3928    0.0\n",
      "3929    0.0\n",
      "3930    0.0\n",
      "3931    0.0\n",
      "3932    0.0\n",
      "3933    0.0\n",
      "3934    0.0\n",
      "3935    0.0\n",
      "3936    0.0\n",
      "3937    0.0\n",
      "3938    0.0\n",
      "3939    0.0\n",
      "3940    0.0\n",
      "3941    0.0\n",
      "3942    0.0\n",
      "3943    0.0\n",
      "3944    0.0\n",
      "3945    0.0\n",
      "3946    0.0\n",
      "3947    0.0\n",
      "3948    0.0\n",
      "3949    0.0\n",
      "3950    0.0\n",
      "3951    0.0\n",
      "3952    0.0\n",
      "Name: 4250, Length: 3706, dtype: float64\n",
      "Prediction Val: [ 3.5325303  -0.37356472  0.8623571  ...  0.25967836 -0.38567734\n",
      "  2.234995  ]\n",
      "0.7380074\n"
     ]
    }
   ],
   "source": [
    "# pd.merge(target, titles, on='MovieID', how='left', suffixes=('_',''))\n",
    "# pick a user and get\n",
    "sample_user = X_test.iloc[99, :]\n",
    "print(\"Sample User:\", sample_user)\n",
    "# get the predicted ratings\n",
    "sample_user_pred = sess.run(output_layer, feed_dict={\n",
    "                            input_layer: [sample_user]})\n",
    "print(\"Prediction Val:\", sample_user_pred[0])\n",
    "print(sample_user_pred[0][3601])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.5325303 , -0.37356472,  0.8623571 , ...,  0.25967836,\n",
       "        -0.38567734,  2.234995  ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_user_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.324612"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sample_user_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
